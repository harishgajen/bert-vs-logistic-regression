{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b50ac9c",
   "metadata": {},
   "source": [
    "# RoBERTa (BERT) vs Logistic Regression\n",
    "## Overview\n",
    "This project will be showcasing the steps to build two different emotion detection NLP models (using RoBERTa and Logistic Regression, as the title suggests). We will also be looking at factors such as the spread of the dataset, time for training, and the accuracy of both models. \n",
    "\n",
    "## Prerequisites\n",
    "If you want to run this project on your own machine, make sure to have the following items installed:\n",
    "\\\n",
    "Python and Jupyter Notebook, Tensorflow, and all Python packages imported below. \n",
    "\\\n",
    "The dataset files ('test.txt', 'train.txt', 'val.txt'), which can be found at the link in the \"Sources\" section at the end of this notebook. Take these files and put them in a subdirectory (called 'data'), relative of the directory of this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676045d",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression\n",
    "We start by building the simpler model, using logistic regression. Before beginning, we import the needed packages for this first part of making the model. Then, we start by importing the dataset into Pandas DataFrames and combining them into one DataFrame, and then preprocessing the text by running some NeatText functions. Then, we'll use scikit-learn to apply Logisitic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59aad9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import neattext.functions as nfx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196dd412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset\n",
    "df1 = pd.read_csv('./data/test.txt', names=['text', 'emotion'], sep=';')\n",
    "df2 = pd.read_csv('./data/train.txt', names=['text', 'emotion'], sep=';')\n",
    "df3 = pd.read_csv('./data/val.txt', names=['text', 'emotion'], sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561408a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cx/yt63_56d0wd52vm5byxp8c3c0000gn/T/ipykernel_12140/4176271390.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df1.append(df2,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df = df1.append(df2,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14dd3b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cx/yt63_56d0wd52vm5byxp8c3c0000gn/T/ipykernel_12140/3193326270.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df3,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df = df.append(df3,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7724ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "df['clean_text'] = df['text'].apply(nfx.remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06b177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(nfx.remove_userhandles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c1bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(nfx.remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14918cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing ML Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a03782be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9a7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = df['clean_text']\n",
    "y_labels = df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93f23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(x_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94e0dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test_lr,y_train,y_test_lr = train_test_split(X,y_labels,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23d245ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harish/programming/emotion-detection/env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bf57c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.885"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.score(x_test_lr,y_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fa58f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "sample_text = [\"I'm a bit mad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d06521ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = cv.transform(sample_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1fd1d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.predict(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db5418c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model and vectorizer\n",
    "pickle.dump(lr_model, open('lr_model.sav', 'wb'))\n",
    "pickle.dump(cv, open('vectorizer.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d7494",
   "metadata": {},
   "source": [
    "## Model 2: RoBERTa\n",
    "We will now following the same series of steps, with changes in our approach, to build a model based off of RoBERTa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4265315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/harish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/harish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import neattext.functions as nfx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a331433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.txt', names=['text', 'emotion'], sep=';')\n",
    "val_data = pd.read_csv('./data/val.txt', names=['text', 'emotion'], sep=';')\n",
    "test_data = pd.read_csv('./data/test.txt', names=['text', 'emotion'], sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afcb7288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values in the dataset\n",
    "train_data['emotion'].isnull().sum()\n",
    "val_data['emotion'].isnull().sum()\n",
    "test_data['emotion'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aa0b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = re.sub('[^A-z]', ' ', text)\n",
    "    negative = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except', 'even though', 'yet']\n",
    "    stop_words = [word for word in stop_words if word not in negative]\n",
    "    preprocessed_tokens = [lemmatizer.lemmatize(contractions.fix(temp.lower())) for temp in text.split() if temp not in stop_words and temp[0] != \"[\"]\n",
    "    return ' '.join([x for x in preprocessed_tokens]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3fce074",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(lambda x: preprocess(x))\n",
    "val_data['text'] = val_data['text'].apply(lambda x: preprocess(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "247a902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding repetition to all classes except the highest frequency class\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "train_x, train_y = ros.fit_resample(np.array(train_data['text']).reshape(-1,1), np.array(train_data['emotion']).reshape(-1,1))\n",
    "train = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text', 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d8aeeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "train_x, train_y = ros.fit_resample(np.array(train_data['text']).reshape(-1, 1), np.array(train_data['emotion']).reshape(-1, 1))\n",
    "train = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text', 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bd0bcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.OneHotEncoder()\n",
    "y_train = le.fit_transform(np.array(train['emotion']).reshape(-1,1)).toarray()\n",
    "y_test = le.fit_transform(np.array(test_data['emotion']).reshape(-1,1)).toarray()\n",
    "y_val = le.fit_transform(np.array(val_data['emotion']).reshape(-1,1)).toarray()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "587d4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bf92d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_encode(data,maximum_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for i in range(len(data.text)):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            data.text[i],\n",
    "            add_special_tokens=True,\n",
    "            max_length=maximum_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1acb8d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/harish/programming/emotion-detection/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(x.split()) for x in train_data['text']])\n",
    "train_input_ids,train_attention_masks = roberta_encode(train,max_len)\n",
    "test_input_ids,test_attention_masks = roberta_encode(test_data,max_len)\n",
    "val_input_ids,val_attention_masks = roberta_encode(val_data,max_len)\n",
    "type(test_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a75f3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Model\n",
    "def create_model(bert_model, max_len):\n",
    "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "    \n",
    "    output = bert_model([input_ids,attention_masks])\n",
    "    output = output[1]\n",
    "    \n",
    "    output = tf.keras.layers.Dense(6,activation='softmax')(output)\n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "    #model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.compile(Adam(learning_rate=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d102badb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 13:23:34.744882: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-13 13:23:34.745082: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFRobertaModel\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0ddcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 43)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 43)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
      " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 43,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 6)            4614        ['tf_roberta_model[0][1]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124,650,246\n",
      "Trainable params: 124,650,246\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(roberta_model, max_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da8c4088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 13:23:37.922495: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-13 13:23:43.905974: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - ETA: 0s - loss: 0.9896 - accuracy: 0.6026"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 13:30:38.450139: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 430s 1s/step - loss: 0.9896 - accuracy: 0.6026 - val_loss: 0.3535 - val_accuracy: 0.8870\n",
      "Epoch 2/4\n",
      "322/322 [==============================] - 420s 1s/step - loss: 0.2612 - accuracy: 0.9092 - val_loss: 0.2445 - val_accuracy: 0.9230\n",
      "Epoch 3/4\n",
      "322/322 [==============================] - 422s 1s/step - loss: 0.1858 - accuracy: 0.9325 - val_loss: 0.2192 - val_accuracy: 0.9290\n",
      "Epoch 4/4\n",
      "322/322 [==============================] - 420s 1s/step - loss: 0.1491 - accuracy: 0.9432 - val_loss: 0.1892 - val_accuracy: 0.9335\n"
     ]
    }
   ],
   "source": [
    "# Training Model (I ran 1 epoch for speed reasons, but increase this to 4+ epochs for higher accuracy)\n",
    "history = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_val),\n",
    "                   epochs=4,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6021a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Weights\n",
    "model.save_weights('roberta_emotion_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f16b22",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "Now that both models have been created, it is time to compare the two! We will start by looking at training time, then graphing the spread of emotions in the dataset, then plotting accuracy graphs of the model, and finally calculating the F1 scores of the two models and comparing their accuracies. \n",
    "\\\n",
    "One factor to consider is training time and resource consumption of the two models. When training the logistic regression model, it only took a matter of seconds, while training four epochs of the RoBERTa-based model took about 26 minutes on my machine. While this is still not a very long time, when using larger datasets, which RoBERTa requires to be more accurate, it can take dozens of hours to train, while logistic regression may just take a higher number of seconds.\n",
    "\\\n",
    "Similarly, when applying the model, logistic regression is much faster and has a lighter load on the machine, while RoBERTa takes a bit of time. Thus, when developing an application that needs to call on the model frequently and return the result to the user, logistic regression would likely be a better choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1de4a54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting graph of dataset spread (number of occurrences of each emotion)\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.countplot(x='emotion',data=df)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08b212de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 13:51:54.328871: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 26s 320ms/step\n"
     ]
    }
   ],
   "source": [
    "# Accuracy and F1 Score\n",
    "result = model.predict([test_input_ids,test_attention_masks])\n",
    "y_pred = np.zeros_like(result)\n",
    "y_pred[np.arange(len(result)),result.argmax(1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b071985f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression model 0.885\n",
      "Accuracy of RoBERTa model 0.9245\n",
      "F1 Score of RoBERTA model: 0.890108061935523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "accuracy_lr = lr_model.score(x_test_lr,y_test_lr)\n",
    "accuracy_roberta = accuracy_score(y_test,y_pred)\n",
    "print('Accuracy of Logistic Regression model', accuracy_lr)\n",
    "print('Accuracy of RoBERTa model', accuracy_roberta)\n",
    "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
    "print('F1 Score of RoBERTA model:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562517ce",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- RoBERTa is a more complex model, that takes longer to train and use, but results in getting more accurate results. \n",
    "- Logistic Regression is faster in both training and applying, but may be a bit less accurate as a tradeoff. \n",
    "- Both models have their benefits and drawbacks, and different usecases may govern which one to go with. If you need a really accurate model that you are periodically going to call, RoBERTa is the one to go with. If you need something fast, logistic regression may be the way to go. \n",
    "\n",
    "Sources:\n",
    "\\\n",
    "Dataset: https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp\n",
    "\\\n",
    "RoBERTa Model: https://www.kaggle.com/code/dhruv1234/emotion-classification-roberta/notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
